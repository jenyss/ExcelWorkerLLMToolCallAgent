{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0279b6-ad82-4fa5-932c-b133ba5c8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph pandas python-dotenv duckdb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3c7fd-3a51-44ae-98c1-0141cf6950b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41378276-49e9-43b7-8e1c-fbf41f1404ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional, Dict, List, Union\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from openai import OpenAI\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958ed90-1b46-4966-8287-4e6ef8e37371",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_GENERATION_PROMPT = \"\"\"You are an Excel analysis expert that generates SQL or Pandas queries to analyze data.\n",
    "\n",
    "IF preview_data IS NOT available:\n",
    "  1. Call load_preview_data to understand Excel content, columns and types\n",
    "  2. Return function call for load_preview_data\n",
    "\n",
    "ELSE IF preview_data IS available:\n",
    "  1. Review the preview_data structure and types\n",
    "  2. Evaluate user_input complexity and generate appropriate query. **VERY IMPORTANT -** Check the **Data Analysis Rules**, **DuckDB SQL Rules* and **Pandas Rules* BEFORE generating a query.\n",
    "  3. Decide and return a function call:\n",
    "      - Call simple_dataframe_query for Pandas queries.\n",
    "      - Call complex_duckdb_query for DuckDB queries.\n",
    "\n",
    "Tools Available:\n",
    "1. load_preview_data: To read the excel and preview its columns and data types. \n",
    "   Input: {\"file_name\": \"example.xlsx\"}\n",
    "\n",
    "2. simple_dataframe_query: For ANY operation that can be easily done with Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"df.query('column > 0').count()\"}\n",
    "   \n",
    "3. complex_duckdb_query: For complex SQL operations (GROUP BY, aggregations). ONLY for operations that cannot be done easily in Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"SELECT * FROM data\"}\n",
    "\n",
    "Data Analysis Rules:\n",
    "1. NULL/Empty Value Handling:\n",
    "   - When calculating averages across columns:\n",
    "     * Sum only non-null values\n",
    "     * Divide by count of non-null values\n",
    "   - Use CAST(column IS NOT NULL AS INTEGER) to count valid values\n",
    "   - Always use NULLIF for safe division\n",
    "   - Handle NULLs before aggregating\n",
    "\n",
    "2. Query Structure Requirements:\n",
    "   - Break complex calculations into CTEs\n",
    "   - Calculate row-level metrics first\n",
    "   - Then group and aggregate\n",
    "   - Include validation counts\n",
    "   - Example:\n",
    "     WITH base_metrics AS (\n",
    "       SELECT *,\n",
    "         COALESCE(value, 0) as clean_value,\n",
    "         CAST(value IS NOT NULL AS INTEGER) as valid_count\n",
    "     ),\n",
    "     row_metrics AS (\n",
    "       SELECT *,\n",
    "         SUM(clean_value) / NULLIF(SUM(valid_count), 0) as row_avg\n",
    "     FROM base_metrics\n",
    "     GROUP BY row_id\n",
    "     )\n",
    "\n",
    "3. DuckDB SQL Rules:\n",
    "   - Quote columns with spaces: \"Column Name\"\n",
    "   - Include all non-aggregated columns in GROUP BY\n",
    "   - Reference table as 'data'\n",
    "   - Use proper type casting\n",
    "   - Add range checks for numeric operations\n",
    "\n",
    "4. Pandas Rules:\n",
    "   - Reference DataFrame as 'df'\n",
    "   - **Numpy quersies are NOT ALLOWED! Only simple Pandas queries are ALLOWED**\n",
    "   - Handle NULLs with fillna()\n",
    "   - Use proper data types\n",
    "\n",
    "Pandas Query Patterns - USE THESE:\n",
    "✓ Filtering: df.query('column == value')\n",
    "✓ Counting: df.query('condition').count()\n",
    "✓ Aggregation: df.query('condition').agg({'col': 'mean'})\n",
    "✓ Multiple conditions: df.query('col1 > 0 and col2 < 10')\n",
    "\n",
    "NEVER USE THESE PATTERNS:\n",
    "✗ df[df['column'] == value]  # No bracket filtering\n",
    "✗ np.any(), np.all()         # No numpy operations\n",
    "✗ df.loc[], df.iloc[]        # Avoid direct indexing\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e562e1c-91cd-4231-bf25-a30ad702cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATE_QUERY_PROMPT = \"\"\"You are an AI assistant validating and improving queries for Excel data analysis.\n",
    "\n",
    "IF query_result contains error then:\n",
    "  1. Analyse the error and given user input: `llm_prompt`, failed query: `query` and `query_result`, generate a new correct query. \n",
    "      - **WHEN query_result contains **error** related to Pandas query execution, then you MUST generate SQL query, given the llm_prompt!**\n",
    "  2. Decide and return a function call:\n",
    "      - Call simple_dataframe_query for Pandas queries.\n",
    "      - Call complex_duckdb_query for DuckDB queries.\n",
    "\n",
    "ELSE IF NO error in query_result then:\n",
    "  1. Verify the `query` agains the `llm_prompt` and ensure that it correctly answers user input.\n",
    "  2. If query corrections are needed to correctly answer the user question then generate a new correct query \n",
    "  3. Decide and return a function call:\n",
    "      - Call simple_dataframe_query for Pandas queries.\n",
    "      - Call complex_duckdb_query for DuckDB queries.\n",
    "\n",
    "Tools Available:\n",
    "2. simple_dataframe_query: For ANY operation that can be easily done with Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"df.query('column > 0').count()\"}\n",
    "   \n",
    "3. complex_duckdb_query: For complex SQL operations (GROUP BY, aggregations). ONLY for operations that cannot be done easily in Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"SELECT * FROM data\"}\n",
    "\n",
    "**SQL Query Validation:**\n",
    "1. NULL Handling:\n",
    "   - Verify NULLs are properly excluded from calculations\n",
    "   - Check for NULLIF in divisions\n",
    "   - Confirm COALESCE usage for NULL defaults\n",
    "   - Validate empty value handling in aggregations\n",
    "\n",
    "2. Aggregation Validation:\n",
    "   - All non-aggregated columns must be in GROUP BY\n",
    "   - Verify aggregation functions match business logic\n",
    "   - Check percentage calculations sum correctly\n",
    "   - Validate count metrics against row counts\n",
    "\n",
    "3. Query Structure:\n",
    "   - Use CTEs for complex calculations\n",
    "   - Break operations into logical steps:\n",
    "     * Clean and validate data\n",
    "     * Calculate row-level metrics\n",
    "     * Perform grouping and aggregation\n",
    "   - Include data validation steps\n",
    "   - Add range checks for numeric operations\n",
    "\n",
    "4. Error Prevention:\n",
    "   - Add NULLIF for division operations\n",
    "   - Include proper type casting\n",
    "   - Validate numeric operations\n",
    "   - Handle edge cases explicitly\n",
    "   \n",
    "5. Complex Calculation Patterns:\n",
    "   WITH data_validation AS (\n",
    "     -- Clean and validate input data\n",
    "   ),\n",
    "   row_metrics AS (\n",
    "     -- Calculate row-level statistics\n",
    "   ),\n",
    "   aggregated_results AS (\n",
    "     -- Perform final aggregations\n",
    "   )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770e853-0858-4b61-a6f4-7a73c28bfa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    query: Optional[str]\n",
    "    file_name: str\n",
    "    preview_data: Optional[Dict[str, List]]\n",
    "    query_result: Optional[Dict]\n",
    "    llm_prompt: Optional[str]\n",
    "    tool: Optional[str]\n",
    "    iterations_count: int\n",
    "    error: Optional[str]\n",
    "    messages: List[Dict]\n",
    "\n",
    "def load_preview_data(file_name: str) -> dict:\n",
    "    try:\n",
    "        if not file_name:\n",
    "            raise ValueError(\"File name must be provided\")\n",
    "        \n",
    "        df = pd.read_excel(os.path.join(os.getcwd(), file_name), nrows=1)\n",
    "        df = df.replace(r'^\\s*$', None, regex=True)\n",
    "        df = df.replace(['nan', 'NaN', 'null'], None)\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        return {\n",
    "            \"columns\": df.columns.tolist(),\n",
    "            \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
    "            \"sample_rows\": df.to_dict(orient=\"records\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to examine Excel structure: {str(e)}\"}\n",
    "\n",
    "def complex_duckdb_query(file_name: str, query: str) -> dict:\n",
    "    try:\n",
    "        df = pd.read_excel(os.path.join(os.getcwd(), file_name))\n",
    "        df = df.replace(r'^\\s*$', None, regex=True)\n",
    "        df = df.replace(['nan', 'NaN', 'null'], None)\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        con = duckdb.connect()\n",
    "        con.register(\"data\", df)\n",
    "        \n",
    "        result = con.execute(query).fetchdf()\n",
    "        if result is None:\n",
    "            return {\"result\": None}\n",
    "        \n",
    "        result = result.replace([float('inf'), -float('inf')], None)\n",
    "        result = result.where(pd.notna(result), None)\n",
    "        \n",
    "        return {\n",
    "            \"result\": {\n",
    "                \"columns\": result.columns.tolist(),\n",
    "                \"rows\": result.to_dict(orient=\"records\")\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"DuckDB query error: {str(e)}\"}\n",
    "    finally:\n",
    "        if 'con' in locals():\n",
    "            con.close()\n",
    "\n",
    "def simple_dataframe_query(file_name: str, query: str) -> dict:\n",
    "    try:\n",
    "        df = pd.read_excel(os.path.join(os.getcwd(), file_name))\n",
    "        df = df.replace(r'^\\s*$', None, regex=True)\n",
    "        df = df.replace(['nan', 'NaN', 'null'], None)\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        safe_globals = {\n",
    "            \"df\": df,\n",
    "            \"pd\": pd,\n",
    "            \"__builtins__\": {}\n",
    "        }\n",
    "        \n",
    "        result = eval(query, safe_globals, {})\n",
    "        \n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            result = result.replace([float('inf'), -float('inf')], None)\n",
    "            result = result.where(pd.notna(result), None)\n",
    "            return {\n",
    "                \"result\": {\n",
    "                    \"type\": \"DataFrame\",\n",
    "                    \"columns\": result.columns.tolist(),\n",
    "                    \"rows\": result.to_dict(orient=\"records\")\n",
    "                }\n",
    "            }\n",
    "        elif isinstance(result, pd.Series):\n",
    "            result = result.replace([float('inf'), -float('inf')], None)\n",
    "            result = result.where(pd.notna(result), None)\n",
    "            return {\n",
    "                \"result\": {\n",
    "                    \"type\": \"Series\",\n",
    "                    \"name\": result.name,\n",
    "                    \"values\": result.tolist()\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"result\": {\n",
    "                    \"type\": \"scalar\",\n",
    "                    \"value\": None if pd.isna(result) else result\n",
    "                }\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error executing query: {str(e)}\"}\n",
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"load_preview_data\",\n",
    "            \"description\": \"Examine Excel file structure\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"file_name\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"file_name\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"simple_dataframe_query\",\n",
    "            \"description\": \"Execute simple Pandas operations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"file_name\": {\"type\": \"string\"},\n",
    "                    \"query\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"file_name\", \"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"complex_duckdb_query\",\n",
    "            \"description\": \"Execute complex SQL operations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"file_name\": {\"type\": \"string\"},\n",
    "                    \"query\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"file_name\", \"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def execute_function(name: str, args: dict, state: AgentState) -> dict:\n",
    "   try:\n",
    "       function_map = {\n",
    "           \"load_preview_data\": load_preview_data,\n",
    "           \"simple_dataframe_query\": simple_dataframe_query,\n",
    "           \"complex_duckdb_query\": complex_duckdb_query\n",
    "       }\n",
    "       result = function_map[name](**args)\n",
    "       \n",
    "       if isinstance(result, dict) and \"error\" in result:\n",
    "           state[\"error\"] = result[\"error\"]\n",
    "           \n",
    "       return result\n",
    "       \n",
    "   except Exception as e:\n",
    "       state[\"error\"] = str(e)\n",
    "       return {\"error\": str(e)}\n",
    "\n",
    "def generate_and_execute_query_node(state: AgentState) -> AgentState:\n",
    "    client = OpenAI()\n",
    "    try:\n",
    "        if not state.get(\"file_name\") or not state.get(\"user_input\"):\n",
    "            raise ValueError(\"Missing required fields: file_name or user_input\")\n",
    "\n",
    "        state[\"iterations_count\"] = state.get(\"iterations_count\", 0) + 1\n",
    "\n",
    "        if \"messages\" not in state:\n",
    "            state[\"messages\"] = []\n",
    "            \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": QUERY_GENERATION_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Preview data: {state.get('preview_data')}\\nFile: {state['file_name']}\\nTask: {state['user_input']}\"}\n",
    "        ]\n",
    "\n",
    "        print(f\"{state.get('iterations_count')} :: Generate Query Node - MESSAGES -> \\n###############################\\n{messages}\\n################################\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            tools=TOOLS,\n",
    "            tool_choice=\"auto\", #if must call then use: required\n",
    "            parallel_tool_calls=True, # Defaults to true!!!!!!!!!!! \n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        print(f\"{state.get('iterations_count')} :: Generate Query Node - LLM RESPONSE -> \\n###############################\\n{response.choices[0].message}\\n################################\")\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        if not message.tool_calls:\n",
    "            raise ValueError(\"No tool calls in response\")\n",
    "            \n",
    "        tool_call = message.tool_calls[0]\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        if \"file_name\" not in args:\n",
    "            args[\"file_name\"] = state[\"file_name\"]\n",
    "            \n",
    "        result = execute_function(tool_call.function.name, args, state)\n",
    "        \n",
    "        state.update({\n",
    "            \"tool\": tool_call.function.name,\n",
    "            \"preview_data\": result if tool_call.function.name == \"load_preview_data\" else state.get(\"preview_data\"),\n",
    "            \"query\": args.get(\"query\"),\n",
    "            \"query_result\": result if tool_call.function.name != \"load_preview_data\" else None,\n",
    "            \"llm_prompt\": state[\"user_input\"][0] if isinstance(state[\"user_input\"], tuple) else state[\"user_input\"],\n",
    "            \"messages\": messages + [\n",
    "                {\"role\": \"assistant\", \"content\": message.content, \"tool_calls\": [{\"id\": tool_call.id, \"type\": \"function\", \"function\": {\"name\": tool_call.function.name, \"arguments\": json.dumps(args)}}]},\n",
    "                {\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"name\": tool_call.function.name, \"content\": json.dumps(result)}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        print(f\"{state.get('iterations_count')} :: Generate Query Node - STATE -> \\n###############################\\n{json.dumps(state, indent=2)}\\n################################\")\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in validate_and_execute_query_node: {str(e)}\")\n",
    "        state[\"error\"] = str(e)\n",
    "        return state\n",
    "        \n",
    "\n",
    "def validate_and_execute_query_node(state: AgentState) -> AgentState:\n",
    "    state[\"iterations_count\"] = state.get(\"iterations_count\", 0) + 1\n",
    "\n",
    "    function_calls = {}\n",
    "    tool_name = state.get(\"tool\")\n",
    "    \n",
    "    if tool_name:\n",
    "        function_calls[tool_name] = function_calls.get(tool_name, 0) + 1  # Safe way to increment\n",
    "    \n",
    "    # Prevent excessive tool calls\n",
    "    if function_calls.get(tool_name, 0) >= 3:\n",
    "        state[\"error\"] = f\"Too many calls to {tool_name}\"\n",
    "        return state\n",
    "    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        if state.get(\"error\"):\n",
    "            return state\n",
    "            \n",
    "        if not state.get(\"query\") or not state.get(\"tool\"):\n",
    "            state[\"error\"] = \"Missing query or tool selection\"\n",
    "            return state\n",
    "\n",
    "        messages = state[\"messages\"] + [\n",
    "            {\"role\": \"system\", \"content\": VALIDATE_QUERY_PROMPT}, \n",
    "            {\"role\": \"user\", \"content\": f\"Preview data: {state.get('preview_data')}\\nLLM query: {state.get('query')}\\nUser query: {state['llm_prompt']}\\nExecution result: {state.get('query_result')}\"}\n",
    "        ]\n",
    "\n",
    "        print(f\"{state.get('iterations_count')} :: Validate Query Node - MESSAGES -> \\n###############################\\n{messages}\\n################################\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            tools=TOOLS,\n",
    "            tool_choice=\"auto\",\n",
    "            parallel_tool_calls=False,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        print(f\"{state.get('iterations_count')} :: Validate Query Node - LLM RESPONSE -> \\n###############################\\n{response.choices[0].message}\\n################################\")\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        if not message.tool_calls:\n",
    "            raise ValueError(\"No tool calls in response, hence the validation passed successfully!\")\n",
    "\n",
    "        # if not message.tool_calls:\n",
    "        #     print(f\"{state.get('iterations_count')} :: No further tool calls, validation complete.\")\n",
    "        #     state[\"messages\"].append(\n",
    "        #         {\"role\": \"assistant\", \"content\": message.content}\n",
    "        #     )\n",
    "        #     return state  # Proceed without executing any tool\n",
    "            \n",
    "        tool_call = message.tool_calls[0]\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        if \"file_name\" not in args:\n",
    "            args[\"file_name\"] = state[\"file_name\"]\n",
    "            \n",
    "        result = execute_function(tool_call.function.name, args, state)\n",
    "        \n",
    "        state[\"query_result\"] = result\n",
    "        \n",
    "        state[\"messages\"].extend([\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": message.content,\n",
    "                \"tool_calls\": [{\"id\": tool_call.id, \"type\": \"function\", \"function\": {\"name\": tool_call.function.name, \"arguments\": json.dumps(args)}}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"name\": tool_call.function.name,\n",
    "                \"content\": json.dumps(result)\n",
    "            }\n",
    "        ])\n",
    "\n",
    "        print(f\"{state.get('iterations_count')} :: Validate Query Node - STATE ->\\n###############################\\n{json.dumps(state, indent=2)}\\n################################\")\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in validate_and_execute_query_node: {str(e)}\")\n",
    "        state[\"error\"] = str(e)\n",
    "        return state\n",
    "\n",
    "def next_step(state: AgentState) -> str:\n",
    "    # Check iterations first\n",
    "    if state.get(\"iterations_count\", 0) > 7:\n",
    "        return END\n",
    "    \n",
    "    if state.get(\"preview_data\") is None:\n",
    "        return \"generate_and_execute_query_node\"\n",
    "        \n",
    "    if state.get(\"query\") is None:\n",
    "        return \"generate_and_execute_query_node\"\n",
    "\n",
    "    if state.get(\"query_result\") is None or (\n",
    "        isinstance(state.get(\"query_result\"), dict) and \n",
    "        state.get(\"query_result\", {}).get(\"error\")\n",
    "    ):\n",
    "        return \"validate_and_execute_query_node\"\n",
    "        \n",
    "    return END\n",
    "\n",
    "# Comment the above next_step and uncomment this one if you want to force the validation step!\n",
    "# def next_step(state: AgentState) -> str:\n",
    "#     # Check iteration limits first\n",
    "#     if state.get(\"iterations_count\", 0) > 7:\n",
    "#         return END\n",
    "\n",
    "#     # Always ensure state updates before progressing\n",
    "#     if state.get(\"preview_data\") is None:\n",
    "#         return \"generate_and_execute_query_node\"\n",
    "        \n",
    "#     if state.get(\"query\") is None:\n",
    "#         return \"generate_and_execute_query_node\"\n",
    "    \n",
    "#     # Ensure that after query generation, validation always happens\n",
    "#     if state.get(\"query_result\") is None or isinstance(state.get(\"query_result\"), dict):\n",
    "#         return \"validate_and_execute_query_node\"\n",
    "        \n",
    "#     return \"validate_and_execute_query_node\"  # Force validation step\n",
    "\n",
    "\n",
    "def create_agent_graph(memory: Optional[MemorySaver] = None) -> StateGraph:\n",
    "    if memory is None:\n",
    "        memory = MemorySaver()\n",
    "    \n",
    "    builder = StateGraph(AgentState)\n",
    "    builder.add_node(\"generate_and_execute_query_node\", generate_and_execute_query_node)\n",
    "    builder.add_node(\"validate_and_execute_query_node\", validate_and_execute_query_node)\n",
    "    \n",
    "    builder.add_conditional_edges(\n",
    "        \"generate_and_execute_query_node\",\n",
    "        next_step,\n",
    "        {\n",
    "            \"generate_and_execute_query_node\": \"generate_and_execute_query_node\", \n",
    "            \"validate_and_execute_query_node\": \"validate_and_execute_query_node\",\n",
    "            END: END # Comment this line to force the validation step\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    builder.add_conditional_edges(\n",
    "        \"validate_and_execute_query_node\",\n",
    "        next_step,\n",
    "        {\n",
    "            \"validate_and_execute_query_node\": \"validate_and_execute_query_node\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    builder.set_entry_point(\"generate_and_execute_query_node\")\n",
    "    return builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "def run_agent_query(user_input: str, file_name: str) -> Dict:\n",
    "    memory = MemorySaver() \n",
    "    workflow = create_agent_graph(memory)\n",
    "    thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "    \n",
    "    state: AgentState = {\n",
    "        \"user_input\": user_input,\n",
    "        \"file_name\": file_name,\n",
    "        \"iterations_count\": 0,\n",
    "        \"query\": None,\n",
    "        \"preview_data\": None,\n",
    "        \"query_result\": None,\n",
    "        \"llm_prompt\": None,\n",
    "        \"tool\": None,\n",
    "        \"error\": None,\n",
    "        \"messages\": []\n",
    "    }\n",
    "\n",
    "    return workflow.invoke(state, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692de968-cbc5-45af-8e6d-cc8a7936806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "graph = create_agent_graph()\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b93d72-b7d7-4afc-995e-46bbdafb688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query\n",
    "user_query = \"Which ticker symbol have avg performance above 50% given the performance of a ticker symbol is spread in the months 1 through 13. Ignore empty months in the calculation!\"\n",
    "file_name = \"ipo_data.xlsx\"\n",
    "\n",
    "final_state = run_agent_query(user_query, file_name)\n",
    "print(json.dumps(final_state, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
